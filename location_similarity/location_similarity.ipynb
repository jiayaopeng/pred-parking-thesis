{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417f08c7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c617354",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders\n",
    "!pip install geopandas\n",
    "!pip install folium\n",
    "!pip install geopy\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. Local imports\n",
    "from setup import load_data_from_s3\n",
    "import location_similarity_helper as lsh\n",
    "import location_similarity_plots as lsp\n",
    "import location_similarity_cluster as lsc\n",
    "import location_similarity_train_evaluate as lste\n",
    "from baseline_helper import create_area_combinations\n",
    "\n",
    "\n",
    "# 1.2. External imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import geopy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely import wkt\n",
    "from sklearn import preprocessing\n",
    "import pprint\n",
    "\n",
    "import importlib\n",
    "importlib.reload(lsp)\n",
    "importlib.reload(lsh)\n",
    "importlib.reload(lsc)\n",
    "importlib.reload(lste)\n",
    "\n",
    "pd.set_option('display.max_column', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34689bc",
   "metadata": {},
   "source": [
    "# 2.Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'bucket-vwfs-pred-park-global-model-serving-dev'\n",
    "file_name = 'input/open_data/seattle/train_data_with_trans_100_with_transaction.csv'\n",
    "train_data_with_trans_100 = load_data_from_s3(bucket_name, file_name)\n",
    "# train_data_with_trans_100 = pd.read_csv('train_data_with_trans_100.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e113d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_trans_100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e2269d",
   "metadata": {},
   "source": [
    "# 3. Filter input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0394e8",
   "metadata": {},
   "source": [
    "As we proved in baseline, that there are 17 areas in total but only 9 areas has a record over 250, and the rest areas are quite spreaded, therefore, we filtered out only the 9 areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ee710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for selected areas\n",
    "selected_areas = [\n",
    "    'Greenlake',\n",
    "    'South Lake Union',\n",
    "    'Commercial Core',\n",
    "    'Pike-Pine',\n",
    "    'Uptown',\n",
    "    'Ballard',\n",
    "    'First Hill',\n",
    "    'Chinatown/ID',\n",
    "    'Pioneer Square'\n",
    "]\n",
    "train_data_with_trans_100_filtered = train_data_with_trans_100[\n",
    "    train_data_with_trans_100[\"study_area\"].isin(selected_areas)\n",
    "]\n",
    "\n",
    "street_count = len(train_data_with_trans_100_filtered.street_id.unique())\n",
    "raw_street_count_unique = len(train_data_with_trans_100.street_id.unique())\n",
    "print(f'Filtered Data Shape: {train_data_with_trans_100_filtered.shape}')\n",
    "print(f'Training data has {street_count} streets')\n",
    "print(f'Original Data without filtering has {raw_street_count_unique} unique streets (ground truth)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd8a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_trans_100_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a610387",
   "metadata": {},
   "source": [
    "# 4. Create Street as Similarity Entity - Calculate Distance Between Street Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db73c8",
   "metadata": {},
   "source": [
    "    1. Goal: understand the similarity of all the streets in seattle based on its vector representation, define the similarity between streets\n",
    "    2. Below steps have been included in this section:\n",
    "\n",
    "        1)Process the data\n",
    "        2)compute the distance(cosine, euclinean)\n",
    "        3)correlate output of the previous, correlate euclinean/cosine withh real geo distance\n",
    "        4)analyze if they are really correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4e9b8",
   "metadata": {},
   "source": [
    "## 4.1 Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_trans_100.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8059ed",
   "metadata": {},
   "source": [
    "hour, weekday, current_capacity, tempC, windspeedKmph, precipMM - we did not include for it to calculate the street similarity as they are time dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "# hour, weekday, current_capacity, tempC, windspeedKmph, precipMM - we did not include for it to calculate the street similarity as they are time dependent\n",
    "selected_features = [\n",
    "    'street_id', # note this is not as feature, but just needed to be selected\n",
    "    'availability', # note this is not as feature, but just needed to be selected\n",
    "    'length',\n",
    "    'highway',\n",
    "    'maxspeed', # input but not used for clustering\n",
    "    'commercial_100',\n",
    "    'residential_100',\n",
    "    'transportation_100',\n",
    "    'schools_100',\n",
    "    'eventsites_100',\n",
    "    'geometry',# note this is not as feature, but just needed to be selected\n",
    "    'restaurant_here_100',\n",
    "    'shopping_here_100',\n",
    "    'office_here_100',\n",
    "    'supermarket_here_100',\n",
    "    'transportation_here_100',\n",
    "    'schools_here_100',\n",
    "    'num_off_street_parking_100',\n",
    "    'off_street_capa_100',\n",
    "    #'ongoing_trans'\n",
    "]\n",
    "# preprocess the data, here we use the df_similarity_features as basis to cluster the streets based on their vector\n",
    "# similarity\n",
    "df_features, df_similarity_features, _, df_geometry = \\\n",
    "    lsh.preprocess_for_similarity_analysis(\n",
    "        train_data_with_trans_100_filtered,\n",
    "        selected_features,\n",
    "        options={\n",
    "            'impute_maxspeed': False,# not use maxspeed when clustering\n",
    "            'encode_highway': True, # use highway when clustering\n",
    "            'time_dependant_features': None, # we decide not to use time_depedent feature when clustering\n",
    "        }\n",
    "    )\n",
    "\n",
    "# only take the unique streets, currently cluster only based on streets\n",
    "df_similarity_features['street_id'] = df_similarity_features.index\n",
    "df_similarity_features.drop_duplicates(subset=['street_id'], inplace=True)\n",
    "df_similarity_features.drop(['street_id'], axis=1, inplace=True)\n",
    "df_geometry = df_geometry \\\n",
    "    .drop_duplicates(subset=['street_id']) \\\n",
    "    .set_index('street_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop on-going transaction, as it is also time dependent feature\n",
    "#df_similarity_features.drop(['ongoing_trans'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d56ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We have used {len(df_similarity_features.columns)} features to cluster the {len(df_similarity_features)} streets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f08ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geometry.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsp.plot_highway(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde28413",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 4.2 Compute the pairwise distance(euclinean, cosine) of the streets based on feature vector\n",
    "hereby we are normalizing the feature:\n",
    "\n",
    "    1)so within each feature, they have the same variation\n",
    "    2)different features have different variation\n",
    "    3)(in which case, we keep the different magnitude of the different features, namely preserved the distribution between features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202dc69",
   "metadata": {},
   "source": [
    "### 4.2.1 Use l2 to normalize features(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141fc023",
   "metadata": {},
   "source": [
    "To calculate the distance between the vectors, we firstly need to normalize the vectors, multiple ways are possible, and we would use L2 below to firstly normalize and then use euclidean to calculate the distance, therefore, we try out different ways and determine a way which could distinguish the streets more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17123b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pairwise distance\n",
    "# for euclinean distance -normalize first, normalize the features NOT the rows\n",
    "similarity_features_l2_normalized = preprocessing.normalize(\n",
    "    df_similarity_features, norm='l2', axis=0)\n",
    "\n",
    "df_similarity_features_l2_normalized = pd.DataFrame(\n",
    "    similarity_features_l2_normalized,\n",
    "    index=df_similarity_features.index,\n",
    "    columns=df_similarity_features.columns\n",
    ")\n",
    "\n",
    "# call the distance function\n",
    "df_pair_dist_l2_normalized = lsh.street_pairwise_dist(\n",
    "    df_similarity_features_l2_normalized, 'euclidean')\n",
    "df_pair_cosine = lsh.street_pairwise_dist(df_similarity_features, 'cosine')\n",
    "\n",
    "# normalize enclinean distance matrix, so that it is on same scale[0, 1] with cosine to be able to compare better with correlation plot\n",
    "\n",
    "df_pair_dist_l2_normalized_scaled = lsh.scale_before_plot_correlation(\n",
    "    df_pair_dist_l2_normalized)\n",
    "\n",
    "# plot all the streets for both l2 normalized and cosine normalized distance\n",
    "# the lighhter the color, the less distant, the more similar they are\n",
    "lsp.plot_distance_matrix(\n",
    "    df_pair_dist_l2_normalized_scaled,\n",
    "    df_pair_cosine,\n",
    "    len(df_similarity_features),\n",
    "    'L2 Normalized Euclidean Distance Matrix Between Street Vectors',\n",
    "    'Cosine Distance Matrix Between Street Vectors'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8da481",
   "metadata": {},
   "source": [
    "### 4.2.2 Use min max to normalize features(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd70a60",
   "metadata": {},
   "source": [
    "Use min-max normalizer first before calculate the euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe464a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use min max scaler\n",
    "similarity_features_max_normalized = preprocessing.normalize(\n",
    "    df_similarity_features, norm='max', axis=0)\n",
    "\n",
    "df_similarity_features_max_normalized = pd.DataFrame(\n",
    "    similarity_features_max_normalized,\n",
    "    index=df_similarity_features.index,\n",
    "    columns=df_similarity_features.columns\n",
    "\n",
    ")\n",
    "# get the euclinean distance\n",
    "df_pair_dist_max_normalized = lsh.street_pairwise_dist(\n",
    "    df_similarity_features_max_normalized, 'euclidean')\n",
    "\n",
    "# get the scaled euclinean distance, range[0,1]\n",
    "df_pair_dist_max_normalized_scaled = lsh.scale_before_plot_correlation(\n",
    "    df_pair_dist_max_normalized)\n",
    "\n",
    "lsp.plot_distance_matrix(\n",
    "    df_pair_dist_max_normalized_scaled,\n",
    "    df_pair_cosine,\n",
    "    len(df_similarity_features),\n",
    "    'Minmax Normalized Euclidean Distance Matrix Between Street Vectors',\n",
    "    'Cosine Distance Matrix Between Street Vectors'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f3fa1",
   "metadata": {},
   "source": [
    "\n",
    "**Conclusion: the darker the color, the more 'distant' are the streets, thus they would be more distinguishable, and therefore minmax normalized euclindean distance can distinguish the streets more**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac1eef",
   "metadata": {},
   "source": [
    "## 4.3 Compute the pairwise distance of the streets based on geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a83994",
   "metadata": {},
   "source": [
    "Another way to think about the distance betweent the streets are of course the geographical distance, and hereby we use the centroid of the lines as the geometry of the streets and calculate the pairwised geographical distance between the streets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0a900",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# get the centroid\n",
    "df_geometry['geometry'] = df_geometry['geometry'].apply(wkt.loads)\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    df_geometry,\n",
    "    geometry=df_geometry['geometry']\n",
    ")\n",
    "gdf['line_centroid'] = gdf['geometry'].centroid\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lsh.calculate_street_similarity_matrix(gdf)\n",
    "df_real_dist = pd.DataFrame(data=result)\n",
    "#df_real_dist.to_csv('df_real_dist.csv')\n",
    "#df_real_dist = pd.read_csv('df_real_dist.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b4da7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 4.4 Correlate the distances calculated above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95a8a8",
   "metadata": {},
   "source": [
    "The goal of this part is to decide which distance metrics to use to calculate the similarity based either on feature vectors of the streets or geometry of the streets, therefore, we correlated the below combinations:\n",
    "1) L2 normalized euclinean distance\n",
    "2) max normalized euclinean distance\n",
    "3) cosine distance respectively with the actual geo distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436024d4",
   "metadata": {},
   "source": [
    "### 4.4.1 Correlation between two distance metrics (euclinean and cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c13071",
   "metadata": {},
   "source": [
    "##### Correlate l2 normalized euclidean and cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e9a61",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "corr_l2euclinean_cosine = df_pair_dist_l2_normalized.corrwith(\n",
    "    df_pair_cosine, axis=0)\n",
    "lsp.plot_correlation_distance(corr_l2euclinean_cosine, 'Correlate L2 Normalized Euclidean and Cosine Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f4603a",
   "metadata": {},
   "source": [
    "##### Correlate Minmax normalized euclidean and cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c48879",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_maxeuclinean_cosine = df_pair_dist_max_normalized.corrwith(\n",
    "    df_pair_cosine, axis=0)\n",
    "\n",
    "lsp.plot_correlation_distance(corr_maxeuclinean_cosine, 'Correlate Minmax Normalized Euclidean and Cosine Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a87a2f",
   "metadata": {},
   "source": [
    "### 4.4.2 Correlation between the l2 or max normalized euclinean distance with the real geo distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e37e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_real_dist.index = df_real_dist.index.astype('int64', False)\n",
    "df_real_dist.columns = df_real_dist.columns.astype('int64', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668db2ac",
   "metadata": {},
   "source": [
    "##### Correlate l2 normalized euclidean distance and real geo distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb99d52",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "corr_l2euclinean_realdist = df_pair_dist_l2_normalized.corrwith(\n",
    "    df_real_dist, axis=0)\n",
    "lsp.plot_correlation_distance(corr_l2euclinean_realdist, 'Correlate L2 Normalized Euclidean and Real Geo Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e13d42",
   "metadata": {},
   "source": [
    "##### Correlate min max normalized euclidean distance and real geo distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acef269",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_maxeuclinean_realdist = df_pair_dist_max_normalized.corrwith(\n",
    "    pd.DataFrame(df_real_dist), axis=0)\n",
    "lsp.plot_correlation_distance(corr_maxeuclinean_realdist, 'Correlate Minmax Normalized Euclidean and Real Geo Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c619a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cosindist_realdist = df_pair_cosine.corrwith(\n",
    "    pd.DataFrame(df_real_dist), axis=0)\n",
    "lsp.plot_correlation_distance(corr_cosindist_realdist , 'Correlate Cosine Distance and Real Geo Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf155b",
   "metadata": {},
   "source": [
    "**Conclusion: based on above graphs, we choose the method which correlated mostly to the real geographica distance, therefore, we decided to use min-max normalized Euclidean distance as street similarity distance metric.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85904ad4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# 5. Create Clusters as Similarity Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66c1de",
   "metadata": {},
   "source": [
    "After creating streets as similarity entity, we now cluster those streets, to generate similarity clusters, which is representive. We have chose several clustering algortims and also initializing the clustering two process for source and target areas separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597864ef",
   "metadata": {},
   "source": [
    "1. use min max normalized euclinean distance to normalize street vectors(based on above analysis)\n",
    "2. for each area combination:\n",
    "        for each type of similarity measurement (either GPS or vector similairity)\n",
    "            cluster data based on different clustering algorithm(either kmeans of ag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c166d",
   "metadata": {},
   "source": [
    "## 5.1 Plot the Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c09309",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_geometry['lon'] = df_geometry.line_centroid.apply(lambda p: p.x)\n",
    "df_geometry['lat'] = df_geometry.line_centroid.apply(lambda p: p.y)\n",
    "\n",
    "df_geometry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e1995",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# get city coordinates\n",
    "\n",
    "city = \"Seattle\"\n",
    "locator = geopy.geocoders.Nominatim(user_agent=\"MyCoder\")\n",
    "location_seattle = locator.geocode(city)\n",
    "\n",
    "location_seattle = [location_seattle.latitude, location_seattle.longitude]\n",
    "print(\"[lat, long]:\", location_seattle)\n",
    "\n",
    "# map to plot the area\n",
    "df_street_coords = df_geometry[['lon', 'lat']]\n",
    "df_study_area = train_data_with_trans_100_filtered[['street_id', 'study_area']]\\\n",
    "    .drop_duplicates()\\\n",
    "    .set_index('street_id')\n",
    "street_coords_study_area = pd.merge(\n",
    "    df_street_coords, df_study_area, left_index=True, right_index=True)\n",
    "\n",
    "street_coords_study_area['study_area'] = street_coords_study_area['study_area'].map(\n",
    "    {\n",
    "\n",
    "        'Pike-Pine': 0,\n",
    "        'First Hill': 1,\n",
    "        'South Lake Union': 2,\n",
    "        'Commercial Core': 3,\n",
    "        'Ballard': 4,\n",
    "        'Chinatown/ID': 5,\n",
    "        'Greenlake': 6,\n",
    "        'Pioneer Square': 7,\n",
    "        'University District': 8,\n",
    "        'Uptown': 9, #\n",
    "        'Uptown Triangle': 10,\n",
    "        'Capitol Hill': 11,\n",
    "        'University District': 12,\n",
    "        '12th Ave': 13,\n",
    "        'Fremont': 14,\n",
    "        'Cherry Hill': 15,\n",
    "        'Ballard Locks': 16,\n",
    "        'Roosevelt': 17,\n",
    "        'Westlake': 18,\n",
    "        'Columbia City': 19\n",
    "    }\n",
    ")\n",
    "\n",
    "# plot the map where shows the 4 districts of Seattle, the output html is called map\n",
    "#lsp.plot_cluster_folium(\n",
    "#    data=street_coords_study_area,\n",
    "#    study_area='study_area',\n",
    "#    tiles='OpenStreetMap'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ef9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_trans_100_filtered.study_area.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(street_coords_study_area.study_area.unique())} unique areas in seattle after filtering out areas with small number of data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce2818",
   "metadata": {},
   "source": [
    "## 5.2 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c19d25",
   "metadata": {},
   "source": [
    "In this section, we cluster the street based on the vector similairity(the triangular matrix of the vector distance) and also on GPS(based on the geometry of the streets)\n",
    "\n",
    "    1)we split the data into train and test, and initialize clustering process for train and test separately\n",
    "    2)we tried out 3 clustering algorithms to generate cluster in result\n",
    "\n",
    "The output is expected to be:\n",
    "\n",
    "    for each street, there is a number of clustering labels generated by using different clustering algorithms and different street similarity(either GPS or vector similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084a379",
   "metadata": {},
   "source": [
    "We have refered to below materials to determine the clustering algorithms:\n",
    "\n",
    "    https://datascience.stackexchange.com/questions/761/clustering-geo-location-coordinates-lat-long-pairs\n",
    "    https://community.dataiku.com/t5/Using-Dataiku-DSS/How-to-cluster-geo-points-according-to-their-pairwise-distances/m-p/2931\n",
    "    https://datascience.stackexchange.com/questions/761/clustering-geo-location-coordinates-lat-long-pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839af898",
   "metadata": {},
   "source": [
    "### 5.2.1 Prepare the clustering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90665b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GPS, we also need to divide by the km_per_radian\n",
    "\n",
    "km_per_radian = 6371.0088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d8d76c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_street_coords = pd.merge(\n",
    "    df_street_coords, df_study_area, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f28dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_street_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d586af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# split train and test area to prepare for clustering\n",
    "\n",
    "all_area_combinations = create_area_combinations(selected_areas)\n",
    "\n",
    "\"\"\"\n",
    "area_input_data:\n",
    "    - first key: area_for_train\n",
    "        - second key: Source, Target\n",
    "        - second key value: dataframe with streets data inside\n",
    "\"\"\"\n",
    "area_input_data = {}\n",
    "for area_combination in all_area_combinations:\n",
    "    area_name = area_combination['Target'][0]\n",
    "    area_input_data[area_name] = {}\n",
    "    area_input_data[area_name]['Target'] = df_street_coords[df_street_coords.study_area == area_name]\n",
    "    train_street_coords_temp=[]\n",
    "    for source_area in area_combination['Source']:\n",
    "        train_street_coords_temp.append(df_street_coords[df_street_coords.study_area == source_area])\n",
    "    area_input_data[area_name]['Source'] = pd.concat(train_street_coords_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb3c42",
   "metadata": {},
   "source": [
    "### 5.2.2 Call Clustering Function & Create Cluster Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loop through different similairty metrics: either sim (represents street vector similairy) or gps(represents GPS coordinates)\n",
    "    loop through the train and test data set(as we have seperate clustering process)\n",
    "        loop through different clustering algorithm\n",
    "\"\"\"\n",
    "\n",
    "#Loop through different similairty metrics\n",
    "for i, area_name in enumerate(area_input_data.keys()):\n",
    "    print(i, '====== area_name:',area_name)\n",
    "    for base in ['sim', 'gps']:\n",
    "        # loop through the train and test data set, in the end append the cluster label back to them\n",
    "        for i, data in enumerate([\n",
    "            area_input_data[area_name]['Source'],\n",
    "            area_input_data[area_name]['Target'],\n",
    "        ]):\n",
    "            is_train = i == 0 # check if it is data is training data or not\n",
    "#            for algorithm in ['db_scan', 'kmeans', 'agg_clustering']:\n",
    "            for algorithm in ['kmeans', 'agg_clustering']:\n",
    "                label = algorithm + \"_label_\" + base  # the name of the label(clustering algo + label + similarity metrics)\n",
    "                print('performing cluster labeling for', label, is_train)\n",
    "                # call the clustering algorithm to generate cluster labels\n",
    "                cluster_data = lsh.create_cluster_label(area_input_data, df_pair_dist_max_normalized,  area_name, base, algorithm, data, is_train)\n",
    "                # append the label back to its dataframe\n",
    "                data[label] = cluster_data\n",
    "\n",
    "                # number of streets in each cluster\n",
    "                result_size = data.groupby(label).size()\n",
    "                print(f'The group result of {result_size}')\n",
    "\n",
    "                # DB scan we need to check the number of outliers\n",
    "                if algorithm == 'db_scan':\n",
    "                    outlier_count = len(data[data[label] == -1])\n",
    "                    outlier_percentage = outlier_count / len(result_size) * 100\n",
    "                    print(\n",
    "                        f'Percentage of data points which has been clustered as outliers: {outlier_percentage}%')\n",
    "\n",
    "                # plot the data with folium\n",
    "                lsp.plot_cluster_folium(\n",
    "                    data=data,\n",
    "                    cluster_label=label,\n",
    "                    train=is_train\n",
    "                )\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75074c1",
   "metadata": {},
   "source": [
    "## 6. Evalution of the Result\n",
    "\n",
    "\n",
    "Steps briefly highlighted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_col = ['kmeans_label_gps']\n",
    "\n",
    "cluster_cols = [\n",
    "    ['agg_clustering_label_gps'],\n",
    "#    ['db_scan_label_gps'],\n",
    "    ['kmeans_label_gps'],\n",
    "#    ['db_scan_label_sim'],\n",
    "    ['kmeans_label_sim'],\n",
    "    ['agg_clustering_label_sim']\n",
    "]\n",
    "\n",
    "feature_col = ['length', 'tempC', 'windspeedKmph', 'precipMM', 'highway', 'hour', 'weekday',\n",
    "               'commercial_100', 'residential_100', 'transportation_100', 'schools_100', 'eventsites_100',\n",
    "               'restaurant_here_100','shopping_here_100', 'office_here_100', 'supermarket_here_100',\n",
    "               'transportation_here_100','schools_here_100',  'current_capacity',\n",
    "               'num_off_street_parking_100',  'off_street_capa_100',  #'ongoing_trans'\n",
    "              ]\n",
    "print(f'There are in total {feature_col} features')\n",
    "\n",
    "target_col = ['availability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the train and test data\n",
    "area_cluster_label = {}\n",
    "for area_combination in all_area_combinations:\n",
    "    area_name = area_combination['Target'][0]\n",
    "    print('processing: ', area_name)\n",
    "    area_cluster_label[area_name] = {}\n",
    "    area_cluster_label[area_name]['Target'] = train_data_with_trans_100_filtered[train_data_with_trans_100_filtered.study_area == area_name]\n",
    "    area_train_temp=[]\n",
    "    for source_area in area_combination['Source']:\n",
    "        area_train_temp.append(train_data_with_trans_100_filtered[train_data_with_trans_100_filtered.study_area == source_area])\n",
    "    area_cluster_label[area_name]['Source'] = pd.concat(area_train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac392bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can only be run once, as we are merging the dataframe\n",
    "\"\"\"\n",
    "below block does the following:\n",
    "    1)for each area split, for each clustering algortihm, get the source and target data, by holding out one area as target are everytime\n",
    "    2)merge the cluster label generated for different streets by different clustering algorithm back to the original dataframe\n",
    "\"\"\"\n",
    "area_source_clusters = {}\n",
    "area_target_clusters = {}\n",
    "for area_combination in all_area_combinations:\n",
    "    area_name = area_combination['Target'][0]\n",
    "    print('processing: ', area_name)\n",
    "\n",
    "\n",
    "    area_input_data[area_name]['Source'] = area_input_data[area_name]['Source'].reset_index()\n",
    "    area_input_data[area_name]['Target'] = area_input_data[area_name]['Target'].reset_index()\n",
    "\n",
    "    area_source_clusters[area_name] = pd.merge(area_cluster_label[area_name]['Source'], area_input_data[area_name]['Source'], on=['street_id', 'study_area'])\n",
    "    area_target_clusters[area_name] = pd.merge(area_cluster_label[area_name]['Target'], area_input_data[area_name]['Target'], on=['street_id', 'study_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ad4eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below block does the following:\n",
    "    1)collect the result for model transfer \n",
    "    2)collect result for Matthew and train cluster data size correlation\n",
    "\"\"\"\n",
    "\n",
    "result_scores = {}\n",
    "area_result_matthew_size_corr = {}\n",
    "area_result_matthew_overfit = {}\n",
    "area_result_feature_importance = {}\n",
    "\n",
    "for area_combination in all_area_combinations:\n",
    "    area_name = area_combination['Target'][0]\n",
    "    print('processing area_name: ', area_name)\n",
    "\n",
    "    result_score, result_matthew_size_corr, result_matthew_overfit, result_feat_importance = lste.train_evaluate_all_approaches(\n",
    "        cluster_cols,\n",
    "        feature_col,\n",
    "        target_col,\n",
    "        area_source_clusters[area_name],\n",
    "        area_target_clusters[area_name],\n",
    "        iterations=1000\n",
    "    )\n",
    "\n",
    "    result_score_df = pd.DataFrame(result_score)\n",
    "    result_scores[area_name] = result_score_df\n",
    "\n",
    "    area_result_matthew_size_corr[area_name] = result_matthew_size_corr\n",
    "    area_result_matthew_overfit[area_name] = result_matthew_overfit\n",
    "\n",
    "    area_result_feature_importance[area_name] = result_feat_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c166b25",
   "metadata": {},
   "source": [
    "### 6.1 Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf262374",
   "metadata": {},
   "outputs": [],
   "source": [
    "for area_combination in all_area_combinations:\n",
    "    area_name = area_combination['Target'][0]\n",
    "    print('area:', area_name)\n",
    "    display(result_scores[area_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc12b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mattew for different area combinations\n",
    "matthews = {}\n",
    "for area_combination in all_area_combinations:\n",
    "    area_name = area_combination['Target'][0]\n",
    "    print('area:', area_name)\n",
    "    #display(result_scores[area_name].loc[['matthews'],:])\n",
    "    matthews[area_name] = result_scores[area_name].loc[['Matthews'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matthews = pd.concat(matthews.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average of matthews across different areas\n",
    "df_matthews.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2328fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matthews.plot.box(figsize=(8, 8), ylim=(-0.3, 0.3), grid=True, yticks=(np.arange(-0.3, 0.3, step=0.1)),\n",
    "                     title='Matthews for Minimum 25 streets in one cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc42633",
   "metadata": {},
   "source": [
    "**As we see from the boxplot above, that our clustering algorithm cannot really beat the baseline and next step we wish to investigate the feature importance and why it is the case that it cannot outperform baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8522eb",
   "metadata": {},
   "source": [
    "### 6.2 Analyze Why clustering method is worse than baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e680e",
   "metadata": {},
   "source": [
    "#### Is it because dataset is too small?\n",
    "\n",
    "by correlating the matthew and train cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2bb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(area_result_matthew_size_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7202247",
   "metadata": {},
   "source": [
    "For the best performing clustering algorithm, we plot the correlation between the training data size and the matthew score on the test cluster in scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af74595",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_matthews = df_matthews.mean(axis=0).to_dict()\n",
    "# get the algorithm name which gives avg best matthews\n",
    "best_algo = [algo for algo, value in dict_matthews.items() if value == df_matthews.mean(axis=0).max()]\n",
    "#best_algo = ['kmeans_label_sim']\n",
    "print(f\"The algorithm which gives the best Matthew is {best_algo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the correlation\n",
    "best_algo_matthew_size_corr = []\n",
    "for area, data in area_result_matthew_size_corr.items():\n",
    "    for algor, values in data.items():\n",
    "        if algor == best_algo[0]:\n",
    "            for test_cluster_no, value in values.items():\n",
    "                best_algo_matthew_size_corr.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f956b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_algo_corr = pd.DataFrame(best_algo_matthew_size_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12625132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_algo_corr.plot.scatter(x='train_cluster_size',\n",
    "                          y='matthew',\n",
    "                          c='DarkBlue',\n",
    "                          figsize=(10, 8),\n",
    "                          title=('Correlation between Matthew Score of Test Cluster and the Size of Its Train '\n",
    "                                'Cluster For Best Algorithm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4fd031",
   "metadata": {},
   "source": [
    "**As we observe from the above plot, we could see that there is a positive correlation between the number of data points in the training cluster, and the mathew score on its matched test cluster.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(area_result_matthew_overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eeef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_algo_matthew_overfit = []\n",
    "for area, data in area_result_matthew_overfit.items():\n",
    "    for algor, values in data.items():\n",
    "        if algor == best_algo[0]:\n",
    "            for label, value in values.items():\n",
    "                best_algo_matthew_overfit.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_algo_matthew_overfit = pd.DataFrame(best_algo_matthew_overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_algo_matthew_overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055252cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim= (df_best_algo_matthew_overfit.to_numpy().min(), df_best_algo_matthew_overfit.to_numpy().max())\n",
    "df_best_algo_matthew_overfit[['train_cluster_matthew', 'valid_cluster_matthew_20', 'test_cluster_matthew']].plot.bar(\n",
    "    rot=0, \n",
    "    figsize=(18,5), \n",
    "    grid=True, \n",
    "    ylim=ylim, \n",
    "    title='Overfiting with in Training Data and Between Train and Test',\n",
    "    xlabel = 'Clusters',\n",
    "    ylabel='Matthew',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_train = df_best_algo_matthew_overfit[['train_cluster_matthew_80', 'valid_cluster_matthew_20']]\n",
    "train_test = df_best_algo_matthew_overfit[['train_cluster_matthew', 'test_cluster_matthew']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc15b6c",
   "metadata": {},
   "source": [
    "#### How badly does the algorithm overfit within the training data?\n",
    "\n",
    "by analyzing Matthew of model by spliting the training cluster into 80% train, 20% valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03246746",
   "metadata": {},
   "source": [
    "overfitting within the train, as we splited train and valid, and they are in the same domain, if there is overfitting effect, then we could conclude that the algorithm works but due to overfit that it cannot demonstrate its ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_within_train =within_train.plot.bar(\n",
    "    rot=0, \n",
    "    figsize=(10, 5), \n",
    "    ylim=ylim, \n",
    "    title='Overfit within Train Clusters by 80-20 Split',\n",
    "    grid=True,\n",
    "    xlabel = 'Clusters',\n",
    "    ylabel='Matthew'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e3d4f",
   "metadata": {},
   "source": [
    "#### How badly does the algorithm overfit?\n",
    "\n",
    "by analyzing Matthew of model on training cluster and test cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae3c7b",
   "metadata": {},
   "source": [
    "Here for the best clustering algorithm, we plot analysize the over fit from train to test clusters which includes overfit + domain shift between the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e89342",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_train_test = train_test.plot.bar(\n",
    "    rot=0, \n",
    "    figsize=(10, 5), \n",
    "    ylim=ylim,\n",
    "    title='Overfit Between Train and Test Clusters',\n",
    "    grid=True,\n",
    "    xlabel = 'Clusters',\n",
    "    ylabel='Matthew',\n",
    "    color = ['green', 'red']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70413d8a",
   "metadata": {},
   "source": [
    "**As we could conclude from above that there is a very severe overfitting when it comes to the train to test model transfer and also maybe domain shift, when it comes to within train clusters 80-20 split, the overfitting trend still exists but not so strong, therefore, we could conclude that the clustering approach could solve partially the problem of domain shift and at the same time suffers the problem of overfitting due to the very small dataset we have.**\n",
    "\n",
    "**As it seems that our clustering approach does not solve the problem of domain shift compeletely, we would like to investigate other approaches which could 1) overcome the problem of small data set and 2) also align the distribution bettween when it comes to model transfer. Therefore, we will investigate domain adaptation approaches in the other notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c3820",
   "metadata": {},
   "source": [
    "### 6.3 Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52502ab4",
   "metadata": {},
   "source": [
    "for the best performing clustering algorithm, here we analyse the feature importance of per cluster for different target areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c379497",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_result_feature_importance['Greenlake']['agg_clustering_label_gps'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_algo_feature_importance = []\n",
    "\n",
    "for area, data in area_result_feature_importance.items():\n",
    "    for algo, values in data.items():\n",
    "        if algo == best_algo[0]:\n",
    "            for label, value in values.items():\n",
    "                best_algo_feature_importance.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_algo_feature_importance[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in best_algo_feature_importance:\n",
    "    if df.index.name != 'Feature Id':\n",
    "        df.set_index('Feature Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b995ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_algo_feat_importance = pd.concat(best_algo_feature_importance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_algo_feat_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb107869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We have used {len(df_best_algo_feat_importance)} features to train our model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a84f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_algo_feat_importance.T.plot.box(figsize=(20, 8), grid=True, title='Feature Importance by Test Cluster', rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c3f5f",
   "metadata": {},
   "source": [
    "**Here we have ignored for different areas, and plot the feature importance on all the test clusters regardless of the area, and test cluster label, we could see that hour and highway, and off-street capacity are two features of importance whereas transportation_100 counts has the lest feature importance**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
