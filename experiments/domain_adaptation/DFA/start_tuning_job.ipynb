{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c27ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "\n",
    "# To use the sagemaker notebook role requires adding some IAM roles to the notebook.\n",
    "# Use same roles as in pred-park-de-sagemaker-dev notebook\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a67885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data must come from S3 (in the same region as the notebook instance is)\n",
    "region = sagemaker_session.boto_region_name \n",
    "\n",
    "# datainput\n",
    "if region == \"eu-west-1\":\n",
    "    seattle = 's3://vwfs-pred-park-irland/input/open_data/seattle/train_data_with_trans_100_with_transaction.csv'\n",
    "    \n",
    "elif region == \"eu-central-1\":\n",
    "    seattle = 's3://bucket-vwfs-pred-park-global-model-serving-dev/input/open_data/seattle/train_data_with_trans_100_with_transaction.csv'\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"Region must be eu-west-1 or eu-central-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e82a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define range for hyperparameters that thaat should be tuned\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.00001, 0.01, scaling_type='Logarithmic'),\n",
    "    #\"weight_decay\": ContinuousParameter(0.001, 0.01, scaling_type='Linear'),\n",
    "    \"batch-size\": CategoricalParameter([16, 32, 64, 128]),\n",
    "    \"optimizer\":CategoricalParameter([\"adam\", \"momentum\"]),\n",
    "    \"include_pbp\": CategoricalParameter([1,0]),\n",
    "    \"use_batchnorm\": CategoricalParameter([1,0]),\n",
    "    #\"loss_func\": CategoricalParameter([\"sigmoid\", \"logistic\"]),\n",
    "    \"hidden_dim\":CategoricalParameter([16,32,64]),\n",
    "    'lambda_1': ContinuousParameter(0.001, 1, scaling_type='Logarithmic'),\n",
    "    'lambda_2': ContinuousParameter(0.1, 100, scaling_type='Logarithmic'),\n",
    "    \"output_dim\": CategoricalParameter([8,16,32,64]) # 19 features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Per default, each instance type can only be launched 2x, so every tuning job should run in a different instance type\n",
    "# instance_list = [\"ml.g4dn.xlarge\", \"ml.g4dn.2xlarge\", \"ml.g4dn.4xlarge\", \"ml.g4dn.8xlarge\"]\n",
    "# digit_tasks = [\"3vs5\", \"8vs3\"]\n",
    "# pi_list = [1, 5, 10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38cc78",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tuning params needs to be in the main file's parser, as input to the parser\n",
    "instance_list = [\"ml.g4dn.xlarge\"]\n",
    "\n",
    "# params which is fixed, other than the default of the main file param\n",
    "fixed_params = {\n",
    "                \"sm_mode\": 1,\n",
    "                \"n_experiments\": 3, # no. of experiments, eg. tuner jobs -> call main file -> no. experiments -> no. of epoch\n",
    "                \"max_epoch\": 200,\n",
    "                \"patience\": 10,\n",
    "                \"early_stop\": 1,\n",
    "                \"source_only\": 0,\n",
    "                \"area_cv\": 1\n",
    "                }\n",
    "# outocome: one tuning and a few training  # source_only\n",
    "job_name = f'Parking-with-adaptat'\n",
    "\n",
    "# define training job-entry point\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"main.py\",  # this script will be executed with above args\n",
    "    role=role,\n",
    "    source_dir='/home/ec2-user/SageMaker/mobility-predpark-global-ML/research/Discriminative-Feature-Alignment/Parking_SimpleDA',  # the whole directly will be uploaded to docker container\n",
    "    framework_version=\"1.4.0\",\n",
    "    py_version=\"py3\",\n",
    "    instance_count=1,\n",
    "    instance_type=instance_list[0],  # new instance type for each tuning job\n",
    "    hyperparameters=fixed_params\n",
    ")\n",
    "# docker container will be launched, copy the whole root(so remove the files which are not needed) and check torch.save code\n",
    "# when docker started, pip install the requirements.txt\n",
    "\n",
    "# scrape target metric from Cloudwatch Logs (print/logging command)\n",
    "# metrix to be optimized\n",
    "objective_metric_name = \"Matthew\" #naming\n",
    "objective_type = \"Maximize\" #direction \n",
    "metric_definitions = [{\"Name\": \"Matthew\", \"Regex\": \"Average Matthew: (.*?);\"}] #maybe star with AUC\n",
    "# regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de603469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator.fit({'seattle': seattle}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692415d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. prep instance (prepaing instances take 3, 4 min usu)\n",
    "2. download training image (pre-defined dockers images, framework_version=\"1.4.0\")\n",
    "3. run all the pip installs for requirements \n",
    " \"/opt/conda/bin/python3.6 main.py --early 1 --max_epoch 10 --n_experiments 1 --patience 10 --sm_mode 1\" fixed params\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc391e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tuning job around the estimator: each this will launch max 200 training jobs\n",
    "# HyperparameterTuner wrapp around the estimator\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=100,  #eg. we have tried 250(with 1 experiments), and 100(with 3 experiments)\n",
    "    max_parallel_jobs=3, # trade off between trainign time and max, not too many parallel, might affect bayesian, each tuner job, will launch one instance\n",
    "    objective_type=objective_type,\n",
    "    base_tuning_job_name=job_name,\n",
    "    strategy='Bayesian', # check\n",
    "    early_stopping_type='Auto'\n",
    ")\n",
    "\n",
    "# start the tuning job, this seattle here looks at the path of the s3 bucket and then put into the opt/ML\n",
    "tuner.fit({'seattle': seattle}, wait=False) # wait, only for notebook see results or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e259cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
